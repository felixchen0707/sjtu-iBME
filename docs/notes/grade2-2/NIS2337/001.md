# 信源与信息熵
## 马尔可夫信源

信源记忆长度为$m+1$时，该时刻发出的信号只与之前的$m$个有关系，则称这种信源为马尔可夫信源。对于一阶马尔可夫信源，类似于下面的式子都成立：

$$
p(x_1\mid x_2,x_3)=p(x_1\mid x_2)
$$

因为$x_1$并不影响$x_3$ 。

## 离散信息熵

### 定义

离散信源信息熵：

$$
H(X)=E[I(X)]=E[-\log p(x_i)]=-\sum_i p(x_i)\log p(x_i)
$$

条件信息熵：

$$
\begin{align}
H(X\mid Y)=\sum_j p(y_j)H(X\mid y_j)&=-\sum_{i,j}p(y_j)p(x_i\mid y_j)\log p(x_i\mid y_j)\\
&=-\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)
\end{align}
$$

联合信息熵：

$$
H(X,Y)=-\sum_{i,j}p(x_i,y_j)\log p(x_i,y_j)
$$

### 重要关系

#### $H(X, Y)=H(Y)+H(X\mid Y)$

$$
H(X, Y)=H(Y)+H(X\mid Y)
$$

证明：

$$
\begin{align}
H(Y)+H(X\mid Y)&=-\sum_j p(y_j)\log p(y_j)-\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)\\
&=-\sum_{i,j} p(x_i,y_j)\log p(y_j)-\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)\\
&=-\sum_{i,j} p(x_i,y_j)\log p(y_j)p(x_i\mid y_j)\\
&=-\sum_{i,j}p(x_i,y_j)\log p(x_i,y_j)=H(X,Y)
\end{align}
$$

推广：

$$
H(X_1,X_2,\cdots ,X_n)=\sum_i^n H(X_i\mid X_1,X_2,\cdots ,X_{i-1})
$$

## 离散互信息

### 定义

$$
I(X;Y)=H(X)-H(X\mid Y)
$$

### 条件互信息定义

$$
\begin{align}
I(X;Y\mid Z)&=H(X\mid Z)-H(X\mid Y,Z)\\
&=\sum_{i,j,k}p(x_i,y_j,z_k)\log \dfrac{p(x_i\mid y_j,z_k)}{p(x_i\mid z_k)}
\end{align}
$$

### 重要关系

#### $I(X;Y)=I(Y;X)$

证明：

$$
\begin{align}
I(X;Y)&=H(X)-H(X\mid Y)\\
&=-\sum_i p(x_i)\log p(x_i)+\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)\\
&=-\sum_{i,j} p(x_i,y_j)\log p(x_i)+\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)\\
&=\sum_{i,j} p(x_i,y_j)\log \dfrac{p(x_i,y_j)}{p(x_i)p(y_j)}=I(Y;X)
\end{align}
$$


#### $0\leqslant I(X;Y)\leqslant H(X)$



#### $I(X;Y,Z)=I(X;Y)+I(X;Z\mid Y)$

证明：

$$
\begin{align}
I(X;Y)+I(X;Z\mid Y)&=\sum_{i,j}p(x_i,y_j)\log \dfrac{p(x_i,y_j)}{p(x_i)p(y_j)}+\sum_{i,j,k}p(x_i,y_j,z_k)\log \dfrac{p(x_i\mid y_j,z_k)}{p(x_i\mid y_j)}\\
&=\sum_{i,j,k}\log \dfrac{p(x_i,y_j)}{p(x_i)p(y_j)}\cdot \dfrac{p(x_i,y_j,z_k)p(y_j)}{p(y_j,z_k)p(x_i,y_j)}\\
&=\sum_{i,j,k}\log \dfrac{p(x_i,y_j,z_k)}{p(x_i)p(y_j,z_k)}=I(X;Y,Z)
\end{align}
$$
#### $I(X;Y,Z)=I(X;Y)$若$X-Y-Z$构成马尔科夫链

证明：

$$
\begin{align}
I(X;Y,Z)&=\sum_{i,j,k}p(x_i.y_j,z_k)\log \dfrac{p(x_i,y_j,z_k)}{p(x_i)p(y_i,z_k)}\\
&=\sum_{i,j,k}p(x_i.y_j,z_k)\log \dfrac{p(x_i\mid y_j,z_k)}{p(x_i)}\\
&=\sum_{i,j,k}p(x_i.y_j,z_k)\log \dfrac{p(x_i\mid y_j)}{p(x_i)}=I(X;Y)
\end{align}
$$

#### 多个随机变量的条件互信息展开

若四个随机变量$W,X,Y,Z$，则互信息展开规则：


$$
\begin{aligned}
I(W;X\mid Y,Z)&=I(W;X,Y\mid Z)-I(W;Y\mid Z)\\
&=I(W,Y;X\mid Z)-I(Y;X\mid Z)
\end{aligned}
$$


## 信息传输不等式

对于构成马尔科夫链的随机变量$X-Y-Z$，他们之间的互信息有关系：

$$
I(X;Y)\geqslant I(X;Z)
$$

证明：

$$
\begin{align}
I(X;Y)-I(X;Z)&=I(X;Y,Z)-I(X;Z)\\
&=I(X;Y\mid Z)\\
&=\sum_k p(z_k)I(X;Y\mid z_k)\geqslant 0.
\end{align}
$$

## 转移概率

对于$m$阶马尔可夫过程，可将在时刻$m$时系统的状态记作$s_i$，经过$n-m$步的转化变成状态$s_j$的转移用状态转移函数来表示：


$$
p(s_j\mid s_i)=p_{ij}(m,n)
$$


对于这个函数，显然具有以下的性质：


$$
p_{ij}(m,n)\geqslant 0,\qquad \sum_{j\in S}p_{ij}(m,n)=1.
$$


若考虑所有状态空间$S=\{s_1,s_2,\dots,s_Q\}$，则可以用转移矩阵来表述，即：


$$
\boldsymbol{P}=\begin{bmatrix}
p_{11}&p_{12}&\cdots&p_{1Q}\\
p_{21}&p_{22}&\cdots&p_{2Q}\\
\vdots&\vdots&\ddots&\vdots\\
p_{Q1}&p_{Q2}&\cdots&p_{QQ}\\
\end{bmatrix}
$$


对于给定的初始输入$\boldsymbol{S}_0={p_{10},p_{20},\dots,p_{Q0}}$，则$k$步转移后，在各个状态的概率可用如下式子表示：


$$
\boldsymbol{P}^{(k)}=\boldsymbol{S}_0\boldsymbol{P}^k
$$

## 连续信源信息熵

### 定义

对于连续分布的信源$X$，其信息熵（又称微分熵）为

$$
h(X)=-\int_{-\infty}^{+\infty}p(x)\log p(x)\ \textrm{d}x,
$$

其中，$p(x)$是信源$X$的分布律。

### 性质

**非负性不成立**：

我们知道对于离散信源信息熵$H(X)$，一定有$H(X)\geqslant 0$。但是对于连续信源信息熵结论不一样。考虑$X\sim U(0,\dfrac{1}{2})$，其微分熵：

$$
h(X)=-\int_{0}^{\frac{1}{2}}2\cdot \log 2\ \mathrm{d}x<0.
$$

可见微分熵不一定是非负的。

**与常数的和的微分熵不变**：

与离散情形相同的，有

$$
h(X+C)=h(X).
$$

其中，$C$是任意的一个常数。

**与常数的积的微分熵改变**：

与离散情形不同的：

$$
h(aX)=h(X)+\log \left |a \right |.
$$

这个结论产生的本质是因为，离散情形下：$p_X(x)=p_Y(ax)$，而连续情形下：$p_X(x)=ap_Y(ax)$.

推而广之，对于由$\boldsymbol{A}\boldsymbol{X}$所代表的联合熵$H(\boldsymbol{A}\boldsymbol{X})$，其中$\boldsymbol{A}$和$\boldsymbol{X}$都是矩阵，有：

$$
h(\boldsymbol{A}\boldsymbol{X})=h(\boldsymbol{X})+\log (\det \boldsymbol{A}).
$$

**微分熵的上界**：

对于给定范围的分布律$X\sim p(x)$，当其是均匀分布时，熵最大；对于给定方差的分布律$Y\sim p(y)$，当其是高斯分布时，熵最大，且为$h(Y)=\dfrac{1}{2}\log (2\pi \mathrm{e}\sigma ^2).$

