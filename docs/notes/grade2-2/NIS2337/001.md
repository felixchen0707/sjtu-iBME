# 信源与信息熵

## 马尔可夫信源

信源记忆长度为$m+1$时，该时刻发出的信号只与之前的$m$个有关系，则称这种信源为马尔可夫信源。对于一阶马尔可夫信源，类似于下面的式子都成立：

$$
p(x_1\mid x_2,x_3)=p(x_1\mid x_2)
$$

因为$x_1$并不影响$x_3$ 。

## 离散信源信息熵

### 定义

离散信源信息熵：

$$
H(X)=E[I(X)]=E[-\log p(x_i)]=-\sum_i p(x_i)\log p(x_i)
$$

条件信息熵：

$$
\begin{align}
H(X\mid Y)=\sum_j p(y_j)H(X\mid y_j)&=-\sum_{i,j}p(y_j)p(x_i\mid y_j)\log p(x_i\mid y_j)\\
&=-\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)
\end{align}
$$

联合信息熵：

$$
H(X,Y)=-\sum_{i,j}p(x_i,y_j)\log p(x_i,y_j)
$$

### 重要关系

#### $H(X, Y)=H(Y)+H(X\mid Y)$

$$
H(X, Y)=H(Y)+H(X\mid Y)
$$

证明：

$$
\begin{align}
H(Y)+H(X\mid Y)&=-\sum_j p(y_j)\log p(y_j)-\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)\\
&=-\sum_{i,j} p(x_i,y_j)\log p(y_j)-\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)\\
&=-\sum_{i,j} p(x_i,y_j)\log p(y_j)p(x_i\mid y_j)\\
&=-\sum_{i,j}p(x_i,y_j)\log p(x_i,y_j)=H(X,Y)
\end{align}
$$

推广：

$$
H(X_1,X_2,\cdots ,X_n)=\sum_i^n H(X_i\mid X_1,X_2,\cdots ,X_{i-1})
$$

## 互信息

### 定义

$$
I(X;Y)=H(X)-H(X\mid Y)
$$

### 条件互信息定义

$$
\begin{align}
I(X;Y\mid Z)&=H(X\mid Z)-H(X\mid Y,Z)\\
&=\sum_{i,j,k}p(x_i,y_j,z_k)\log \dfrac{p(x_i\mid y_j,z_k)}{p(x_i\mid z_k)}
\end{align}
$$

### 重要关系

#### $I(X;Y)=I(Y;X)$

证明：

$$
\begin{align}
I(X;Y)&=H(X)-H(X\mid Y)\\
&=-\sum_i p(x_i)\log p(x_i)+\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)\\
&=-\sum_{i,j} p(x_i,y_j)\log p(x_i)+\sum_{i,j}p(x_i,y_j)\log p(x_i\mid y_j)\\
&=\sum_{i,j} p(x_i,y_j)\log \dfrac{p(x_i,y_j)}{p(x_i)p(y_j)}=I(Y;X)
\end{align}
$$


#### $0\leqslant I(X;Y)\leqslant H(X)$



#### $I(X;Y,Z)=I(X;Y)+I(X;Z\mid Y)$

证明：

$$
\begin{align}
I(X;Y)+I(X;Z\mid Y)&=\sum_{i,j}p(x_i,y_j)\log \dfrac{p(x_i,y_j)}{p(x_i)p(y_j)}+\sum_{i,j,k}p(x_i,y_j,z_k)\log \dfrac{p(x_i\mid y_j,z_k)}{p(x_i\mid y_j)}\\
&=\sum_{i,j,k}\log \dfrac{p(x_i,y_j)}{p(x_i)p(y_j)}\cdot \dfrac{p(x_i,y_j,z_k)p(y_j)}{p(y_j,z_k)p(x_i,y_j)}\\
&=\sum_{i,j,k}\log \dfrac{p(x_i,y_j,z_k)}{p(x_i)p(y_j,z_k)}=I(X;Y,Z)
\end{align}
$$
#### $I(X;Y,Z)=I(X;Y)$若$X-Y-Z$构成马尔科夫链

证明：

$$
\begin{align}
I(X;Y,Z)&=\sum_{i,j,k}p(x_i.y_j,z_k)\log \dfrac{p(x_i,y_j,z_k)}{p(x_i)p(y_i,z_k)}\\
&=\sum_{i,j,k}p(x_i.y_j,z_k)\log \dfrac{p(x_i\mid y_j,z_k)}{p(x_i)}\\
&=\sum_{i,j,k}p(x_i.y_j,z_k)\log \dfrac{p(x_i\mid y_j)}{p(x_i)}=I(X;Y)
\end{align}
$$

#### 多个随机变量的条件互信息展开

若四个随机变量$W,X,Y,Z$，则互信息展开规则：


$$
\begin{aligned}
I(W;X\mid Y,Z)&=I(W;X,Y\mid Z)-I(W;Y\mid Z)\\
&=I(W,Y;X\mid Z)-I(Y;X\mid Z)
\end{aligned}
$$


## 信息传输不等式

对于构成马尔科夫链的随机变量$X-Y-Z$，他们之间的互信息有关系：

$$
I(X;Y)\geqslant I(X;Z)
$$

证明：

$$
\begin{align}
I(X;Y)-I(X;Z)&=I(X;Y,Z)-I(X;Z)\\
&=I(X;Y\mid Z)\\
&=\sum_k p(z_k)I(X;Y\mid z_k)\geqslant 0.
\end{align}
$$

## 转移概率

对于$m$阶马尔可夫过程，可将在时刻$m$时系统的状态记作$s_i$，经过$n-m$步的转化变成状态$s_j$的转移用状态转移函数来表示：


$$
p(s_j\mid s_i)=p_{ij}(m,n)
$$


对于这个函数，显然具有以下的性质：


$$
p_{ij}(m,n)\geqslant 0,\qquad \sum_{j\in S}p_{ij}(m,n)=1.
$$


若考虑所有状态空间$S=\{s_1,s_2,\dots,s_Q\}$，则可以用转移矩阵来表述，即：


$$
\boldsymbol{P}=\begin{bmatrix}
p_{11}&p_{12}&\cdots&p_{1Q}\\
p_{21}&p_{22}&\cdots&p_{2Q}\\
\vdots&\vdots&\ddots&\vdots\\
p_{Q1}&p_{Q2}&\cdots&p_{QQ}\\
\end{bmatrix}
$$


对于给定的初始输入$\boldsymbol{S}_0={p_{10},p_{20},\dots,p_{Q0}}$，则$k$步转移后，在各个状态的概率可用如下式子表示：


$$
\boldsymbol{P}^{(k)}=\boldsymbol{S}_0\boldsymbol{P}^k
$$


